# architect

Herramienta CLI headless y agÃ©ntica para orquestar agentes de IA sobre archivos locales y servicios MCP remotos. DiseÃ±ada para funcionar sin supervisiÃ³n en CI, cron y pipelines.

---

## InstalaciÃ³n

**Requisitos**: Python 3.12+

```bash
# Desde el repositorio
git clone https://github.com/tu-usuario/architect-cli
cd architect-cli
pip install -e .

# Verificar instalaciÃ³n
architect --version
architect run --help
```

**Dependencias principales**: `litellm`, `click`, `pydantic`, `httpx`, `structlog`, `tenacity`

---

## Quickstart

```bash
# Configurar API key
export LITELLM_API_KEY="sk-..."

# Analizar un proyecto (solo lectura, seguro)
architect run "resume quÃ© hace este proyecto" -a resume

# Revisar cÃ³digo
architect run "revisa main.py y encuentra problemas" -a review

# Generar un plan detallado (sin modificar archivos)
architect run "planifica cÃ³mo aÃ±adir tests al proyecto" -a plan

# Modificar archivos â€” build planifica y ejecuta en un solo paso
architect run "aÃ±ade docstrings a todas las funciones de utils.py"

# Ejecutar sin confirmaciones (CI/automatizaciÃ³n)
architect run "genera un archivo README.md para este proyecto" --mode yolo

# Ver quÃ© harÃ­a sin ejecutar nada
architect run "reorganiza la estructura de carpetas" --dry-run

# Limitar tiempo total de ejecuciÃ³n
architect run "refactoriza el mÃ³dulo de auth" --timeout 300
```

---

## Comandos

### `architect run` â€” ejecutar tarea

```
architect run PROMPT [opciones]
```

**Argumento**:
- `PROMPT` â€” DescripciÃ³n de la tarea en lenguaje natural

**Opciones principales**:

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `-c, --config PATH` | Archivo de configuraciÃ³n YAML |
| `-a, --agent NAME` | Agente a usar: `plan`, `build`, `resume`, `review`, o custom |
| `-m, --mode MODE` | Modo de confirmaciÃ³n: `confirm-all`, `confirm-sensitive`, `yolo` |
| `-w, --workspace PATH` | Directorio de trabajo (workspace root) |
| `--dry-run` | Simular ejecuciÃ³n sin cambios reales |

**Opciones LLM**:

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `--model MODEL` | Modelo a usar (`gpt-4o`, `claude-sonnet-4-6`, etc.) |
| `--api-base URL` | URL base de la API |
| `--api-key KEY` | API key directa |
| `--no-stream` | Desactivar streaming |
| `--timeout N` | Tiempo mÃ¡ximo total de ejecuciÃ³n en segundos (watchdog global) |

**Opciones de output**:

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `-v / -vv / -vvv` | Nivel de verbose tÃ©cnico (sin `-v` solo se muestran los pasos del agente) |
| `--log-level LEVEL` | Nivel de log: `human` (default), `debug`, `info`, `warn`, `error` |
| `--log-file PATH` | Guardar logs JSON estructurados en archivo |
| `--json` | Salida en formato JSON (compatible con `jq`) |
| `--quiet` | Modo silencioso (solo resultado final en stdout) |
| `--max-steps N` | LÃ­mite mÃ¡ximo de pasos del agente |
| `--budget N` | LÃ­mite de coste en USD (detiene el agente si se supera) |

**Opciones de evaluaciÃ³n**:

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `--self-eval off\|basic\|full` | Auto-evaluaciÃ³n del resultado: `off` (sin coste extra), `basic` (una llamada extra, marca como `partial` si falla), `full` (reintenta con prompt de correcciÃ³n hasta `max_retries` veces) |

**Opciones MCP**:

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `--disable-mcp` | Desactivar conexiÃ³n a servidores MCP |

---

### `architect agents` â€” listar agentes

```bash
architect agents                   # agentes por defecto
architect agents -c config.yaml   # incluye custom del YAML
```

Lista todos los agentes disponibles con su modo de confirmaciÃ³n.

---

### `architect validate-config` â€” validar configuraciÃ³n

```bash
architect validate-config -c config.yaml
```

Valida la sintaxis y los valores del archivo de configuraciÃ³n antes de ejecutar.

---

## Agentes

Un agente define el **rol**, las **tools disponibles** y el **nivel de confirmaciÃ³n**.

El agente por defecto es **`build`** (se usa automÃ¡ticamente si no se especifica `-a`): analiza el proyecto, elabora un plan interno y lo ejecuta en un solo paso, sin necesitar un agente `plan` previo.

| Agente | DescripciÃ³n | Tools | ConfirmaciÃ³n | Pasos |
|--------|-------------|-------|-------------|-------|
| `build` | Planifica y ejecuta modificaciones | todas (ediciÃ³n, bÃºsqueda, lectura, `run_command`) | `confirm-sensitive` | 50 |
| `plan` | Analiza y genera un plan detallado | `read_file`, `list_files`, `search_code`, `grep`, `find_files` | `yolo` | 20 |
| `resume` | Lee y resume informaciÃ³n | `read_file`, `list_files`, `search_code`, `grep`, `find_files` | `yolo` | 15 |
| `review` | RevisiÃ³n de cÃ³digo y mejoras | `read_file`, `list_files`, `search_code`, `grep`, `find_files` | `yolo` | 20 |

**Agentes custom** en `config.yaml`:

```yaml
agents:
  deploy:
    system_prompt: |
      Eres un agente de deployment...
    allowed_tools:
      - read_file
      - list_files
      - run_command
    confirm_mode: confirm-all
    max_steps: 10
```

---

## Modos de confirmaciÃ³n

| Modo | Comportamiento |
|------|---------------|
| `confirm-all` | Toda acciÃ³n requiere confirmaciÃ³n interactiva |
| `confirm-sensitive` | Solo acciones que modifican el sistema (write, delete) |
| `yolo` | EjecuciÃ³n completamente automÃ¡tica (para CI/scripts) |

> En entornos sin TTY (`--mode confirm-sensitive` en CI), el sistema lanza un error claro. Usa `--mode yolo` o `--dry-run` en pipelines.

---

## ConfiguraciÃ³n

Copia `config.example.yaml` como punto de partida:

```bash
cp config.example.yaml config.yaml
```

Estructura mÃ­nima:

```yaml
llm:
  model: gpt-4o-mini          # o claude-sonnet-4-6, ollama/llama3, etc.
  api_key_env: LITELLM_API_KEY
  timeout: 60
  retries: 2
  stream: true

workspace:
  root: .
  allow_delete: false

logging:
  level: human                 # human (default), debug, info, warn, error
  verbose: 0
```

### Variables de entorno

| Variable | Equivalente config | DescripciÃ³n |
|----------|--------------------|-------------|
| `LITELLM_API_KEY` | `llm.api_key_env` | API key del proveedor LLM |
| `ARCHITECT_MODEL` | `llm.model` | Modelo LLM |
| `ARCHITECT_API_BASE` | `llm.api_base` | URL base de la API |
| `ARCHITECT_LOG_LEVEL` | `logging.level` | Nivel de logging |
| `ARCHITECT_WORKSPACE` | `workspace.root` | Directorio de trabajo |

---

## Salida y cÃ³digos de salida

**SeparaciÃ³n stdout/stderr**:
- Streaming del LLM â†’ **stderr** (no rompe pipes)
- Logs y progreso â†’ **stderr**
- Resultado final del agente â†’ **stdout**
- `--json` output â†’ **stdout**

```bash
# Parsear resultado con jq
architect run "resume el proyecto" --quiet --json | jq .status

# Capturar resultado, ver logs
architect run "analiza main.py" -v 2>logs.txt

# Solo resultado (sin logs)
architect run "genera README" --quiet --mode yolo
```

**CÃ³digos de salida**:

| CÃ³digo | Significado |
|--------|-------------|
| `0` | Ã‰xito (`success`) |
| `1` | Fallo del agente (`failed`) |
| `2` | Parcial â€” hizo algo pero no completÃ³ (`partial`) |
| `3` | Error de configuraciÃ³n |
| `4` | Error de autenticaciÃ³n LLM |
| `5` | Timeout |
| `130` | Interrumpido (Ctrl+C) |

---

## Formato JSON (`--json`)

```bash
architect run "analiza el proyecto" -a review --quiet --json
```

```json
{
  "status": "success",
  "stop_reason": null,
  "output": "El proyecto consiste en...",
  "steps": 3,
  "tools_used": [
    {"name": "list_files", "success": true},
    {"name": "read_file", "path": "src/main.py", "success": true}
  ],
  "duration_seconds": 8.5,
  "model": "gpt-4o-mini",
  "costs": {"total_usd": 0.0023, "prompt_tokens": 4200, "completion_tokens": 380}
}
```

**`stop_reason`**: indica por quÃ© terminÃ³ el agente. `null` = terminÃ³ naturalmente. Otros valores: `max_steps`, `timeout`, `budget_exceeded`, `context_full`, `user_interrupt`, `llm_error`.

Cuando un watchdog activa (`max_steps`, `timeout`, etc.), el agente recibe una instrucciÃ³n de cierre y hace una Ãºltima llamada al LLM para resumir quÃ© completÃ³ y quÃ© queda pendiente antes de terminar.

---

## Logging

Por defecto, architect muestra los pasos del agente en un formato legible con iconos:

```
ğŸ”„ Paso 1 â†’ Llamada al LLM (6 mensajes)
   âœ“ LLM respondiÃ³ con 2 tool calls

   ğŸ”§ read_file â†’ src/main.py
      âœ“ OK

   ğŸ”§ edit_file â†’ src/main.py (3â†’5 lÃ­neas)
      âœ“ OK
      ğŸ” Hook ruff: âœ“

ğŸ”„ Paso 2 â†’ Llamada al LLM (10 mensajes)
   âœ“ LLM respondiÃ³ con texto final

âœ… Agente completado (2 pasos)
   RazÃ³n: LLM decidiÃ³ que terminÃ³
   Coste: $0.0042
```

Las tools MCP se distinguen visualmente: `ğŸŒ mcp_github_search â†’ query (MCP: github)`

```bash
# Solo pasos legibles (default â€” nivel HUMAN)
architect run "..."

# Nivel HUMAN + logs tÃ©cnicos por step
architect run "..." -v

# Detalle completo (args, respuestas LLM)
architect run "..." -vv

# Todo (HTTP, payloads)
architect run "..." -vvv

# Sin logs (resultado solo)
architect run "..." --quiet

# Logs a archivo JSON + consola
architect run "..." -v --log-file logs/session.jsonl

# Analizar logs despuÃ©s
cat logs/session.jsonl | jq 'select(.event == "tool.call")'
```

**Pipelines de logging independientes**:
- **HUMAN** (stderr, default): pasos, tool calls, hooks â€” formato legible con iconos, sin ruido tÃ©cnico
- **TÃ©cnico** (stderr, con `-v`): debug de LLM, tokens, retries â€” excluye mensajes HUMAN
- **JSON file** (archivo, con `--log-file`): todos los eventos estructurados

Ver [`docs/logging.md`](docs/logging.md) para detalles de la arquitectura de logging.

---

## Post-Edit Hooks

Los hooks se ejecutan automÃ¡ticamente despuÃ©s de cada operaciÃ³n de ediciÃ³n (`edit_file`, `write_file`, `apply_patch`). El resultado se aÃ±ade al contexto del agente para que pueda corregir errores.

```yaml
hooks:
  post_edit:
    - name: ruff
      command: "ruff check {file} --fix"
      file_patterns: ["*.py"]
      timeout: 15

    - name: mypy
      command: "mypy {file} --ignore-missing-imports"
      file_patterns: ["*.py"]
      timeout: 30

    - name: prettier
      command: "prettier --write {file}"
      file_patterns: ["*.ts", "*.tsx", "*.json"]
      timeout: 10
```

- `{file}` se reemplaza por el path del archivo editado
- TambiÃ©n disponible como variable de entorno `ARCHITECT_EDITED_FILE`
- Un hook que falla (exit code != 0) devuelve su output al LLM como feedback

---

## Control de costes

```yaml
costs:
  budget_usd: 2.0         # Detiene el agente si supera $2
  warn_at_usd: 1.5        # Avisa en logs al llegar a $1.5
```

```bash
# LÃ­mite de presupuesto por CLI
architect run "..." --budget 1.0
```

El coste acumulado aparece en el output `--json` bajo `costs`. Cuando se supera el presupuesto, el agente recibe una instrucciÃ³n de cierre y hace un Ãºltimo resumen antes de terminar (`stop_reason: "budget_exceeded"`).

---

## MCP (Model Context Protocol)

Conecta architect a herramientas remotas vÃ­a HTTP:

```yaml
mcp:
  servers:
    - name: github
      url: http://localhost:3001
      token_env: GITHUB_TOKEN

    - name: database
      url: https://mcp.example.com/db
      token_env: DB_TOKEN
```

Las tools MCP se descubren automÃ¡ticamente al iniciar y son indistinguibles de las tools locales para el agente. Si un servidor no estÃ¡ disponible, el agente continÃºa sin esas tools.

```bash
# Con MCP
architect run "abre un PR con los cambios" --mode yolo

# Sin MCP
architect run "analiza el proyecto" --disable-mcp
```

---

## Uso en CI/CD

```yaml
# GitHub Actions
- name: Refactorizar cÃ³digo
  run: |
    architect run "actualiza los imports obsoletos en src/" \
      --mode yolo \
      --quiet \
      --json \
      --budget 3.0 \
      -c ci/architect.yaml \
    | tee result.json

- name: Verificar resultado
  run: |
    STATUS=$(cat result.json | jq -r .status)
    if [ "$STATUS" != "success" ]; then
      echo "architect fallÃ³ con status: $STATUS ($(cat result.json | jq -r .stop_reason))"
      exit 1
    fi
```

```yaml
# config para CI (ci/architect.yaml)
llm:
  model: gpt-4o-mini
  api_key_env: OPENAI_API_KEY
  retries: 3
  timeout: 120

workspace:
  root: .

logging:
  level: human
  verbose: 0

hooks:
  post_edit:
    - name: lint
      command: "ruff check {file} --fix"
      file_patterns: ["*.py"]
```

---

## Seguridad

- **Path traversal**: todas las operaciones de archivos estÃ¡n confinadas al `workspace.root`. Intentos de acceder a `../../etc/passwd` son bloqueados.
- **delete_file** requiere `workspace.allow_delete: true` explÃ­cito en config.
- **run_command**: lista de comandos bloqueados (`rm -rf`, `dd`, `mkfs`, etc.) y whitelist de comandos de desarrollo seguros. El directorio de trabajo estÃ¡ siempre confinado al workspace.
- **Tools MCP** son marcadas como sensibles por defecto (requieren confirmaciÃ³n en `confirm-sensitive`).
- **API keys** nunca se loggean, solo el nombre de la variable de entorno.

---

## Proveedores LLM soportados

Cualquier proveedor soportado por [LiteLLM](https://docs.litellm.ai/docs/providers):

```bash
# OpenAI
LITELLM_API_KEY=sk-... architect run "..." --model gpt-4o

# Anthropic
LITELLM_API_KEY=sk-ant-... architect run "..." --model claude-sonnet-4-6

# Google Gemini
LITELLM_API_KEY=... architect run "..." --model gemini/gemini-2.0-flash

# Ollama (local, sin API key)
architect run "..." --model ollama/llama3 --api-base http://localhost:11434

# LiteLLM Proxy (para equipos)
architect run "..." --api-base http://proxy.internal:8000
```

---

## Arquitectura

```
architect run PROMPT
    â”‚
    â”œâ”€â”€ load_config()          YAML + env vars + CLI flags
    â”œâ”€â”€ configure_logging()    3 pipelines: HUMAN + tÃ©cnico + JSON file
    â”œâ”€â”€ ToolRegistry           tools locales (fs, ediciÃ³n, bÃºsqueda, run_command) + MCP remotas
    â”œâ”€â”€ RepoIndexer            Ã¡rbol del workspace â†’ inyectado en system prompt
    â”œâ”€â”€ LLMAdapter             LiteLLM con retries selectivos + prompt caching
    â”œâ”€â”€ ContextManager         pruning: compress + enforce_window + is_critically_full
    â”œâ”€â”€ PostEditHooks          lint/test automÃ¡tico post-ediciÃ³n
    â”œâ”€â”€ CostTracker            coste acumulado + watchdog de presupuesto
    â”‚
    â””â”€â”€ AgentLoop (while True â€” el LLM decide cuÃ¡ndo parar)
            â”‚
            â”œâ”€â”€ _check_safety_nets()   max_steps / budget / timeout / context_full
            â”‚       â””â”€â”€ si salta â†’ _graceful_close(): Ãºltima LLM call sin tools
            â”‚                         el agente resume quÃ© hizo y quÃ© queda pendiente
            â”œâ”€â”€ context_manager.manage()     compress + enforce_window si necesario
            â”œâ”€â”€ llm.completion()             â†’ streaming chunks a stderr
            â”œâ”€â”€ si no hay tool_calls         â†’ LLM_DONE, fin natural
            â”œâ”€â”€ engine.execute_tool_calls()  â†’ paralelo si posible â†’ confirmar â†’ ejecutar
            â”œâ”€â”€ engine.run_post_edit_hooks() â†’ lint/test â†’ feedback al LLM si falla
            â””â”€â”€ repetir
```

**Razones de parada** (`stop_reason` en el output JSON):

| RazÃ³n | DescripciÃ³n |
|-------|-------------|
| `null` / `llm_done` | El LLM decidiÃ³ que terminÃ³ (terminaciÃ³n natural) |
| `max_steps` | Watchdog: lÃ­mite de pasos alcanzado |
| `budget_exceeded` | Watchdog: lÃ­mite de coste superado |
| `context_full` | Watchdog: context window lleno (>95%) |
| `timeout` | Watchdog: tiempo total excedido |
| `user_interrupt` | El usuario hizo Ctrl+C / SIGTERM (corte inmediato) |
| `llm_error` | Error irrecuperable del LLM |

**Decisiones de diseÃ±o**:
- Sync-first (predecible, debuggable; el loop principal es ~300 lÃ­neas sin magia)
- Sin LangChain/LangGraph (el loop es directo y controlado)
- Pydantic v2 como fuente de verdad para schemas y validaciÃ³n
- Errores de tools devueltos al LLM como resultado (no rompen el loop)
- stdout limpio para pipes, todo lo demÃ¡s a stderr
- Watchdogs piden cierre limpio â€” el agente nunca termina a mitad de frase

---

## Historial de versiones

| VersiÃ³n | Funcionalidad |
|---------|---------------|
| v0.9.0 | **EdiciÃ³n incremental**: `edit_file` (str-replace exacto) y `apply_patch` (unified diff) |
| v0.10.0 | **Indexer + bÃºsqueda**: Ã¡rbol del repo en el system prompt, `search_code`, `grep`, `find_files` |
| v0.11.0 | **Context management**: truncado de tool results, compresiÃ³n de pasos con LLM, hard limit, parallel tool calls |
| v0.12.0 | **Self-evaluation**: `--self-eval basic/full` evalÃºa y reintenta automÃ¡ticamente |
| v0.13.0 | **`run_command`**: ejecuciÃ³n de comandos (tests, linters) con 4 capas de seguridad |
| v0.14.0 | **Cost tracking**: `CostTracker`, `--budget`, prompt caching, `LocalLLMCache` |
| v0.15.0 | **v3-core** â€” rediseÃ±o del nÃºcleo: `while True` loop, safety nets con cierre limpio, `PostEditHooks`, nivel de log HUMAN, `StopReason`, `ContextManager.manage()` |
| v0.15.2 | **Human logging con iconos** â€” formato visual alineado con plan v3: ğŸ”„ğŸ”§ğŸŒâœ…âš¡âŒğŸ“¦ğŸ”, distinciÃ³n MCP, eventos nuevos (`llm_response`), coste en completado |
| v0.15.3 | **Fix pipeline structlog** â€” human logging funciona sin `--log-file`; `wrap_for_formatter` siempre activo |
