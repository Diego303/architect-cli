# Plan de ImplementaciÃ³n v3 â€” Core RediseÃ±ado

Este plan reemplaza las fases F2 (LLM + Loop), F3 (Agentes), F5 (Logging), F7 (Robustez) y F11 (Token optimization) de los planes anteriores. Las demÃ¡s fases (F0, F1, F4, F9, F10, F12, F13, F14) siguen vigentes con ajustes menores que se documentan al final.

---

## Cambios Respecto a v1/v2

| QuÃ© cambia | v1/v2 (antes) | v3 (ahora) |
|-------------|---------------|------------|
| Agent loop | `for step in range(max_steps)` | `while True` â€” el LLM decide cuÃ¡ndo parar |
| TerminaciÃ³n | Counter + timeout | LLM deja de pedir tools = fin natural |
| Safety nets | Cortan abruptamente | Inyectan instrucciÃ³n de cierre â†’ Ãºltima llamada al LLM |
| Context | Crece sin lÃ­mite (F11 lo arreglaba) | `ContextManager` integrado desde el core |
| Plan + Build | Dos fases secuenciales rÃ­gidas | Plan integrado en el prompt de build |
| MixedModeRunner | Clase separada | Eliminado â€” build planifica internamente |
| Log levels | `debug \| info \| warn \| error` | + `human` como nivel de trazabilidad legible |
| Logs human | No existÃ­an | Iconos + formato legible para seguir al agente |
| Auto-verify | El agente decide manualmente | Hook en el Engine: lint/test automÃ¡tico post-edit |
| Cierre en lÃ­mites | `state.status = "partial"` frÃ­o | LLM resume quÃ© hizo y quÃ© queda pendiente |

---

## Estructura de Archivos Actualizada (solo cambios)

```
src/architect/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ loop.py              # REESCRITO â€” while True + safety nets
â”‚   â”œâ”€â”€ state.py             # AMPLIADO â€” nuevos estados de cierre
â”‚   â”œâ”€â”€ context.py           # REESCRITO â€” ContextManager con budget de tokens
â”‚   â””â”€â”€ hooks.py             # NUEVO â€” post-edit verification hooks
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ prompts.py           # REESCRITO â€” plan integrado en build
â”‚   â””â”€â”€ registry.py          # SIMPLIFICADO â€” sin MixedModeRunner
â””â”€â”€ logging/
    â”œâ”€â”€ setup.py             # AMPLIADO â€” pipeline human
    â”œâ”€â”€ human.py             # NUEVO â€” formateador de logs humanos
    â””â”€â”€ levels.py            # NUEVO â€” nivel HUMAN custom
```

---

## MEJORA 1 â€” Agent Loop RediseÃ±ado (`while True`)

### El Principio

> El flujo natural de un agente es: el LLM trabaja hasta que decide que terminÃ³.
> Los lÃ­mites (max_steps, budget, timeout, context) son watchdogs.
> Cuando un watchdog salta, no corta â€” pide un cierre limpio.

### 1.1 â€” El Nuevo Loop

```python
# src/architect/core/loop.py
import time
from enum import Enum

class StopReason(Enum):
    """Por quÃ© se detuvo el agente."""
    LLM_DONE = "llm_done"              # El LLM decidiÃ³ que terminÃ³ (natural)
    MAX_STEPS = "max_steps"            # Watchdog: lÃ­mite de pasos
    BUDGET_EXCEEDED = "budget_exceeded" # Watchdog: lÃ­mite de coste
    CONTEXT_FULL = "context_full"      # Watchdog: context window lleno
    TIMEOUT = "timeout"                # Watchdog: tiempo total excedido
    USER_INTERRUPT = "user_interrupt"   # El usuario hizo Ctrl+C
    LLM_ERROR = "llm_error"           # Error irrecuperable del LLM


class AgentLoop:
    def __init__(
        self,
        llm: LLMAdapter,
        engine: ExecutionEngine,
        agent_config: AgentConfig,
        context_mgr: ContextManager,
        cost_tracker: CostTracker | None,
        shutdown: GracefulShutdown,
        logger: structlog.BoundLogger,
        timeout: int | None = None,
    ):
        self.llm = llm
        self.engine = engine
        self.config = agent_config
        self.ctx = context_mgr
        self.costs = cost_tracker
        self.shutdown = shutdown
        self.log = logger
        self.timeout = timeout
        self._start_time: float = 0

    def run(self, prompt: str) -> AgentState:
        self._start_time = time.time()
        state = AgentState()
        state.messages = self.ctx.build_initial(self.config, prompt)
        tools_schema = self.engine.registry.get_schemas(
            self.config.allowed_tools or None
        )
        step = 0

        while True:
            # â”€â”€â”€ SAFETY CHECKS (antes de cada step) â”€â”€â”€
            stop = self._check_safety_nets(state, step)
            if stop is not None:
                return self._graceful_close(state, stop)

            # â”€â”€â”€ CONTEXT MANAGEMENT (antes de cada llamada al LLM) â”€â”€â”€
            state.messages = self.ctx.manage(state.messages)

            # â”€â”€â”€ LLAMADA AL LLM â”€â”€â”€
            self.log.msg(
                "llm.call",
                step=step,
                messages_count=len(state.messages),
                _level="human",
            )

            try:
                response = self.llm.completion(
                    messages=state.messages,
                    tools=tools_schema if tools_schema else None,
                )
            except Exception as e:
                self.log.error("llm.error", error=str(e), step=step)
                # Intentar recuperarse: retry ya lo hizo el adapter.
                # Si llega aquÃ­ es irrecuperable.
                state.status = "failed"
                state.stop_reason = StopReason.LLM_ERROR
                state.final_output = f"Error irrecuperable del LLM: {e}"
                return state

            # Registrar coste
            if self.costs and response.usage:
                self.costs.record(
                    step=step,
                    model=self.llm.config.model,
                    usage=response.usage,
                )

            step += 1

            # â”€â”€â”€ EL LLM DECIDIÃ“ TERMINAR â”€â”€â”€
            # (respondiÃ³ con texto, sin pedir tools)
            if not response.tool_calls:
                self.log.msg(
                    "agent.done",
                    step=step,
                    reason="llm_decided",
                    _level="human",
                )
                state.final_output = response.content
                state.status = "success"
                state.stop_reason = StopReason.LLM_DONE
                return state

            # â”€â”€â”€ EL LLM PIDIÃ“ TOOLS â†’ EJECUTAR â”€â”€â”€
            state.messages.append(self._assistant_message(response))

            tool_results = []
            for tc in response.tool_calls:
                self.log.msg(
                    "tool.call",
                    tool=tc.name,
                    args_summary=self._summarize_args(tc.arguments),
                    _level="human",
                )

                result = self.engine.execute_tool_call(tc.name, tc.arguments)

                self.log.msg(
                    "tool.result",
                    tool=tc.name,
                    success=result.success,
                    _level="human",
                )

                tool_results.append(ToolCallResult(
                    tool_name=tc.name,
                    args=tc.arguments,
                    result=result,
                ))

                # â”€â”€â”€ AUTO-VERIFY POST-EDIT â”€â”€â”€
                if tc.name in ("edit_file", "write_file", "apply_patch"):
                    verify_result = self.engine.run_post_edit_hooks(
                        tc.name, tc.arguments
                    )
                    if verify_result:
                        tool_results.append(verify_result)

            # AÃ±adir resultados al contexto
            state.messages = self.ctx.append_tool_results(
                state.messages, response.tool_calls, tool_results
            )

            # Registrar step
            state.steps.append(StepResult(
                step_number=step,
                llm_response=response,
                tool_calls_made=tool_results,
                timestamp=time.time(),
            ))

    # â”€â”€â”€ SAFETY NETS â”€â”€â”€

    def _check_safety_nets(self, state: AgentState, step: int) -> StopReason | None:
        """
        Comprueba todas las condiciones de seguridad.
        Retorna None si todo OK, o el StopReason si hay que parar.
        """
        # 1. User interrupt (Ctrl+C)
        if self.shutdown.should_stop:
            return StopReason.USER_INTERRUPT

        # 2. Max steps
        if step >= self.config.max_steps:
            self.log.msg(
                "safety.max_steps",
                step=step,
                max=self.config.max_steps,
                _level="human",
            )
            return StopReason.MAX_STEPS

        # 3. Budget
        if self.costs and self.costs.over_budget:
            self.log.msg(
                "safety.budget",
                spent=self.costs.total_cost_usd,
                budget=self.costs.budget_usd,
                _level="human",
            )
            return StopReason.BUDGET_EXCEEDED

        # 4. Timeout
        if self.timeout and (time.time() - self._start_time) > self.timeout:
            self.log.msg("safety.timeout", _level="human")
            return StopReason.TIMEOUT

        # 5. Context window (si despuÃ©s de comprimir sigue lleno)
        if self.ctx.is_critically_full(state.messages):
            return StopReason.CONTEXT_FULL

        return None

    # â”€â”€â”€ CIERRE LIMPIO â”€â”€â”€

    def _graceful_close(self, state: AgentState, reason: StopReason) -> AgentState:
        """
        Cuando un safety net salta, no cortamos de golpe.
        Le damos al LLM una Ãºltima oportunidad de cerrar con un resumen.
        """
        self.log.msg(
            "agent.closing",
            reason=reason.value,
            _level="human",
        )

        close_instructions = {
            StopReason.MAX_STEPS: (
                "Has alcanzado el lÃ­mite mÃ¡ximo de pasos. "
                "Responde con:\n"
                "1. Un resumen de lo que completaste\n"
                "2. QuÃ© queda pendiente\n"
                "3. Sugerencias para continuar"
            ),
            StopReason.BUDGET_EXCEEDED: (
                "Se ha alcanzado el presupuesto mÃ¡ximo de tokens/coste. "
                "Resume lo que completaste y quÃ© falta por hacer."
            ),
            StopReason.CONTEXT_FULL: (
                "El contexto de conversaciÃ³n estÃ¡ lleno. "
                "Resume lo que completaste y quÃ© falta por hacer."
            ),
            StopReason.TIMEOUT: (
                "Se agotÃ³ el tiempo asignado. "
                "Resume lo que completaste y quÃ© falta por hacer."
            ),
            StopReason.USER_INTERRUPT: None,  # No llamar al LLM si el usuario cancela
        }

        instruction = close_instructions.get(reason)

        if instruction:
            # Una Ãºltima llamada SIN tools para que el LLM cierre
            state.messages.append({
                "role": "user",
                "content": f"[SISTEMA] {instruction}",
            })
            try:
                response = self.llm.completion(
                    messages=state.messages,
                    tools=None,  # Sin tools â€” solo texto de cierre
                )
                state.final_output = response.content
            except Exception:
                state.final_output = (
                    f"El agente se detuvo por: {reason.value}. "
                    f"Pasos completados: {len(state.steps)}."
                )
        else:
            state.final_output = (
                f"Interrumpido por el usuario. "
                f"Pasos completados: {len(state.steps)}."
            )

        state.status = "partial"
        state.stop_reason = reason
        return state
```

### 1.2 â€” AgentState Actualizado

```python
# src/architect/core/state.py
from dataclasses import dataclass, field
from typing import Literal

@dataclass
class AgentState:
    messages: list[dict] = field(default_factory=list)
    steps: list[StepResult] = field(default_factory=list)
    status: Literal["running", "success", "partial", "failed"] = "running"
    stop_reason: StopReason | None = None
    final_output: str | None = None

    @property
    def current_step(self) -> int:
        return len(self.steps)

    def to_output_dict(self) -> dict:
        return {
            "status": self.status,
            "stop_reason": self.stop_reason.value if self.stop_reason else None,
            "output": self.final_output,
            "steps_completed": len(self.steps),
            "tools_used": [
                {
                    "step": s.step_number,
                    "tool": tc.tool_name,
                    "success": tc.result.success,
                }
                for s in self.steps
                for tc in s.tool_calls_made
            ],
        }
```

### 1.3 â€” Por QuÃ© Este DiseÃ±o Es Correcto

```
ANTES (v1):                         AHORA (v3):

for i in range(max_steps):          while True:
    response = llm(...)                 if watchdog_triggered:
    if done: break                          graceful_close()  â† LLM resume
    execute_tools()                         break
else:                                   response = llm(...)
    status = "partial"  â† frÃ­o          if no tool_calls:
                                            done!  â† LLM decidiÃ³
                                            break
                                        execute_tools()
```

La diferencia real: el `for-range` hace que `max_steps` sea la estructura. El `while True` hace que **la decisiÃ³n del LLM** sea la estructura y `max_steps` sea un guardia.

---

## MEJORA 2 â€” Context Management Integrado en el Core

### El Principio

> El context window es un recurso finito. Si no lo gestionas activamente,
> cualquier tarea de >8 pasos explota. Esto no es optimizaciÃ³n â€” es necesidad funcional.

### 2.1 â€” ContextManager (reemplaza al simple ContextBuilder)

```python
# src/architect/core/context.py
import structlog

class ContextManager:
    """
    Gestiona el contexto del agente: construcciÃ³n, mediciÃ³n, truncado y compresiÃ³n.
    Integrado en el core desde el dÃ­a 1 â€” no es una optimizaciÃ³n posterior.
    """

    # â”€â”€â”€ ConfiguraciÃ³n â”€â”€â”€

    # Tokens estimados por carÃ¡cter (aprox para cÃ³digo/inglÃ©s/espaÃ±ol)
    CHARS_PER_TOKEN = 4

    # MÃ¡ximo de tokens que un resultado de tool puede ocupar
    MAX_TOOL_RESULT_TOKENS = 2000

    # Activar compresiÃ³n cuando el contexto supera este % del mÃ¡ximo
    COMPRESS_THRESHOLD = 0.75

    # Siempre mantener los Ãºltimos N pasos completos sin comprimir
    KEEP_RECENT_STEPS = 4

    # Contexto mÃ¡ximo (ajustar segÃºn modelo)
    MAX_CONTEXT_TOKENS = 100_000

    def __init__(self, config: ContextConfig | None = None, llm: LLMAdapter | None = None):
        self.log = structlog.get_logger()
        self.llm = llm  # Necesario para resumir (puede ser None si no se quiere)
        if config:
            self.MAX_TOOL_RESULT_TOKENS = config.max_tool_result_tokens
            self.COMPRESS_THRESHOLD = config.compress_threshold
            self.KEEP_RECENT_STEPS = config.keep_recent_steps
            self.MAX_CONTEXT_TOKENS = config.max_context_tokens

    # â”€â”€â”€ ConstrucciÃ³n de contexto inicial â”€â”€â”€

    def build_initial(self, agent_config: AgentConfig, prompt: str,
                      repo_index: RepoIndex | None = None) -> list[dict]:
        """Construye los mensajes iniciales: system + user."""
        system_parts = [agent_config.system_prompt]

        # Inyectar contexto del repo si existe
        if repo_index:
            system_parts.append(self._format_repo_context(repo_index))

        return [
            {"role": "system", "content": "\n\n".join(system_parts)},
            {"role": "user", "content": prompt},
        ]

    def _format_repo_context(self, index: RepoIndex) -> str:
        return (
            "## Estructura del proyecto\n"
            f"Archivos: {index.total_files} | "
            f"LÃ­neas: {index.total_lines} | "
            f"Lenguajes: {', '.join(f'{k}({v})' for k,v in index.languages.items())}\n\n"
            f"```\n{index.tree_summary}\n```\n\n"
            "Usa search_code o grep para encontrar cÃ³digo relevante "
            "antes de hacer cambios."
        )

    # â”€â”€â”€ AÃ±adir resultados de tools â”€â”€â”€

    def append_tool_results(
        self,
        messages: list[dict],
        tool_calls: list,
        results: list[ToolCallResult],
    ) -> list[dict]:
        """AÃ±ade resultados de tools al contexto, truncando si son muy largos."""
        tool_messages = []
        for tc, result in zip(tool_calls, results):
            # Truncar resultados largos ANTES de aÃ±adirlos
            output = self._truncate_tool_result(result.result.output)

            tool_messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "content": output if result.result.success
                    else f"ERROR: {result.result.error}\n{output}",
            })

        return messages + tool_messages

    # â”€â”€â”€ Truncado de resultados de tools â”€â”€â”€

    def _truncate_tool_result(self, output: str) -> str:
        """Trunca resultados largos preservando inicio y final."""
        estimated = self._estimate_tokens(output)
        if estimated <= self.MAX_TOOL_RESULT_TOKENS:
            return output

        lines = output.splitlines()
        if len(lines) <= 10:
            # Texto corto pero denso â€” truncar por caracteres
            max_chars = self.MAX_TOOL_RESULT_TOKENS * self.CHARS_PER_TOKEN
            return output[:max_chars] + "\n\n[... truncado ...]"

        # Mantener primeras 60% y Ãºltimas 25% de las lÃ­neas permitidas
        max_lines = max(20, len(lines) * self.MAX_TOOL_RESULT_TOKENS // estimated)
        head_lines = int(max_lines * 0.6)
        tail_lines = int(max_lines * 0.25)
        omitted = len(lines) - head_lines - tail_lines

        head = "\n".join(lines[:head_lines])
        tail = "\n".join(lines[-tail_lines:])
        return f"{head}\n\n[... {omitted} lÃ­neas omitidas ...]\n\n{tail}"

    # â”€â”€â”€ GestiÃ³n del contexto (llamar antes de cada LLM call) â”€â”€â”€

    def manage(self, messages: list[dict]) -> list[dict]:
        """
        Pipeline de gestiÃ³n de contexto. Se llama antes de cada llamada al LLM.

        1. Medir uso actual
        2. Si supera threshold â†’ comprimir pasos antiguos
        3. Si sigue excediendo â†’ sliding window duro
        """
        current_tokens = self._estimate_total_tokens(messages)
        threshold = int(self.MAX_CONTEXT_TOKENS * self.COMPRESS_THRESHOLD)

        if current_tokens <= threshold:
            return messages  # Todo bien, no hacer nada

        self.log.info(
            "context.compress",
            current_tokens=current_tokens,
            threshold=threshold,
        )

        # Paso 1: Comprimir pasos antiguos
        compressed = self._compress_old_steps(messages)
        new_tokens = self._estimate_total_tokens(compressed)

        if new_tokens <= self.MAX_CONTEXT_TOKENS:
            return compressed

        # Paso 2: Hard limit â€” mantener solo los Ãºltimos N intercambios
        return self._hard_truncate(compressed)

    def _compress_old_steps(self, messages: list[dict]) -> list[dict]:
        """
        Comprime pasos antiguos en un resumen.
        Mantiene: system + user original + resumen + Ãºltimos N pasos.
        """
        if len(messages) < 6:
            return messages  # Nada que comprimir

        system_msg = messages[0]
        user_msg = messages[1]

        # Calcular cuÃ¡ntos mensajes son "recientes" (N pasos Ã— ~3 msgs por paso)
        recent_count = self.KEEP_RECENT_STEPS * 3
        if len(messages) - 2 <= recent_count:
            return messages  # No hay suficientes pasos viejos

        old_messages = messages[2:-recent_count]
        recent_messages = messages[-recent_count:]

        # Generar resumen
        summary = self._generate_summary(old_messages)

        return [
            system_msg,
            user_msg,
            {"role": "assistant", "content": f"[Resumen de pasos anteriores]\n{summary}"},
            *recent_messages,
        ]

    def _generate_summary(self, messages: list[dict]) -> str:
        """
        Resume pasos antiguos. Usa el LLM si estÃ¡ disponible,
        sino hace un resumen mecÃ¡nico (extracto de tool calls).
        """
        if self.llm:
            try:
                resp = self.llm.completion([
                    {"role": "system", "content": (
                        "Resume las siguientes acciones del agente en un pÃ¡rrafo conciso. "
                        "Incluye: archivos leÃ­dos/modificados, quÃ© se intentÃ³, "
                        "quÃ© funcionÃ³ y quÃ© fallÃ³. MÃ¡ximo 150 palabras."
                    )},
                    {"role": "user", "content": self._extract_actions_text(messages)},
                ])
                return resp.content
            except Exception:
                pass  # Fallback al resumen mecÃ¡nico

        return self._mechanical_summary(messages)

    def _mechanical_summary(self, messages: list[dict]) -> str:
        """Resumen sin LLM: extrae tool calls y sus resultados."""
        actions = []
        for msg in messages:
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                for tc in msg["tool_calls"]:
                    actions.append(f"- {tc['function']['name']}({self._brief_args(tc)})")
            elif msg.get("role") == "tool":
                content = msg.get("content", "")
                status = "OK" if not content.startswith("ERROR") else "ERROR"
                actions.append(f"  â†’ {status}")

        return "Acciones previas:\n" + "\n".join(actions[-30:])  # Ãšltimas 30

    def _hard_truncate(self, messages: list[dict]) -> list[dict]:
        """Ãšltimo recurso: mantener system + user + Ãºltimos N mensajes."""
        system_msg = messages[0]
        user_msg = messages[1]
        # Ir cortando hasta que quepa
        for keep in range(len(messages) - 2, 2, -3):
            candidate = [system_msg, user_msg] + messages[-keep:]
            if self._estimate_total_tokens(candidate) <= self.MAX_CONTEXT_TOKENS:
                return candidate
        # Caso extremo: solo system + user
        return [system_msg, user_msg]

    # â”€â”€â”€ MediciÃ³n â”€â”€â”€

    def _estimate_tokens(self, text: str) -> int:
        return len(text) // self.CHARS_PER_TOKEN

    def _estimate_total_tokens(self, messages: list[dict]) -> int:
        total = 0
        for msg in messages:
            if isinstance(msg.get("content"), str):
                total += self._estimate_tokens(msg["content"])
            if msg.get("tool_calls"):
                total += self._estimate_tokens(str(msg["tool_calls"]))
        return total

    def is_critically_full(self, messages: list[dict]) -> bool:
        """True si el contexto estÃ¡ al 95% incluso despuÃ©s de comprimir."""
        return self._estimate_total_tokens(messages) > int(self.MAX_CONTEXT_TOKENS * 0.95)
```

### 2.2 â€” Config del ContextManager

```python
# En config/schema.py
class ContextConfig(BaseModel):
    max_tool_result_tokens: int = 2000
    compress_threshold: float = 0.75     # Comprimir al 75% de uso
    keep_recent_steps: int = 4           # Mantener Ãºltimos 4 pasos
    max_context_tokens: int = 100_000    # Ajustar segÃºn modelo
```

```yaml
# En config.yaml
context:
  max_tool_result_tokens: 2000
  compress_threshold: 0.75
  keep_recent_steps: 4
  max_context_tokens: 100000
```

---

## MEJORA 3 â€” Plan Integrado en Build

### El Principio

> Claude Code no tiene una "fase plan" â†’ "fase build". El agente planifica,
> ejecuta, verifica y re-planifica todo en el mismo loop. Es mÃ¡s natural.

### 3.1 â€” Eliminar MixedModeRunner

**Borrar completamente** la clase `MixedModeRunner` del plan v1. No se necesita.

### 3.2 â€” Nuevo Prompt de Build (con planificaciÃ³n integrada)

```python
# src/architect/agents/prompts.py

BUILD_PROMPT = """Eres un agente de desarrollo de software. Trabajas de forma metÃ³dica y verificas tu trabajo.

## Tu proceso de trabajo

1. ANALIZAR: Lee los archivos relevantes y entiende el contexto antes de actuar
2. PLANIFICAR: Piensa en los pasos necesarios y el orden correcto
3. EJECUTAR: Haz los cambios paso a paso
4. VERIFICAR: DespuÃ©s de cada cambio, comprueba que funciona
5. CORREGIR: Si algo falla, analiza el error y corrÃ­gelo

## Reglas

- Siempre lee un archivo antes de editarlo
- Usa search_code o grep para encontrar cÃ³digo relevante en vez de adivinar
- Para cambios pequeÃ±os, usa edit_file (reemplazar texto exacto)
- Para cambios mÃºltiples dispersos, usa apply_patch (unified diff)
- Solo usa write_file para archivos nuevos o reescrituras completas
- Si un comando o test falla, analiza el error e intenta corregirlo
- Cuando hayas completado la tarea, explica quÃ© hiciste y quÃ© archivos cambiaste

## Importante

- NO pidas confirmaciÃ³n ni hagas preguntas â€” actÃºa con la informaciÃ³n disponible
- Si no tienes suficiente informaciÃ³n, busca en el cÃ³digo antes de asumir
- Haz el mÃ­nimo de cambios necesarios para completar la tarea"""


PLAN_PROMPT = """Eres un agente de anÃ¡lisis y planificaciÃ³n. Tu trabajo es entender una tarea
y producir un plan detallado SIN ejecutar cambios.

## Tu proceso

1. Lee los archivos relevantes para entender el contexto
2. Analiza quÃ© cambios son necesarios
3. Produce un plan estructurado con:
   - QuÃ© archivos hay que crear/modificar/borrar
   - QuÃ© cambios concretos en cada archivo
   - En quÃ© orden hacerlos
   - Posibles riesgos o dependencias

## Reglas

- NO modifiques ningÃºn archivo
- Usa read_file, search_code, grep y list_files para investigar
- SÃ© especÃ­fico: no digas "modificar auth.py", di "en auth.py, aÃ±adir validaciÃ³n de token en la funciÃ³n validate() lÃ­nea ~45"
- Si algo es ambiguo, indica las opciones y recomienda una"""


RESUME_PROMPT = """Eres un agente de anÃ¡lisis y resumen. Tu trabajo es leer informaciÃ³n
y producir un resumen claro y conciso. No modificas archivos.

SÃ© directo. No repitas lo que ya sabe el usuario. CÃ©ntrate en lo importante."""


REVIEW_PROMPT = """Eres un agente de revisiÃ³n de cÃ³digo. Tu trabajo es inspeccionar cÃ³digo
y dar feedback constructivo y accionable.

## QuÃ© buscar
- Bugs y errores lÃ³gicos
- Problemas de seguridad
- Oportunidades de simplificaciÃ³n
- Code smells y violaciones de principios SOLID
- Tests que faltan

## Reglas
- NO modifiques ningÃºn archivo
- SÃ© especÃ­fico: indica archivo, lÃ­nea y el problema concreto
- Prioriza: primero bugs/seguridad, luego mejoras, luego estilo"""
```

### 3.3 â€” Nuevo Agente Default

Cuando el usuario ejecuta `architect run "..."` **sin especificar `-a`**, se usa directamente el agente `build`:

```python
# src/architect/agents/registry.py

DEFAULT_AGENTS = {
    "build": AgentConfig(
        system_prompt=BUILD_PROMPT,
        allowed_tools=[
            "read_file", "write_file", "edit_file", "apply_patch",
            "delete_file", "list_files", "search_code", "grep",
            "find_files", "run_command",
        ],
        confirm_mode="confirm-sensitive",
        max_steps=50,  # â† MÃ¡s holgado porque el LLM decide cuÃ¡ndo parar
    ),
    "plan": AgentConfig(
        system_prompt=PLAN_PROMPT,
        allowed_tools=["read_file", "list_files", "search_code", "grep", "find_files"],
        confirm_mode="yolo",  # Plan no modifica nada, no necesita confirmar
        max_steps=20,
    ),
    "resume": AgentConfig(
        system_prompt=RESUME_PROMPT,
        allowed_tools=["read_file", "list_files", "search_code", "grep", "find_files"],
        confirm_mode="yolo",
        max_steps=15,
    ),
    "review": AgentConfig(
        system_prompt=REVIEW_PROMPT,
        allowed_tools=["read_file", "list_files", "search_code", "grep", "find_files"],
        confirm_mode="yolo",
        max_steps=20,
    ),
}

DEFAULT_AGENT = "build"
```

Nota: `max_steps=50` parece alto, pero recuerda que ahora es un watchdog, no el driver del loop. Claude Code en modo interactivo **no tiene lÃ­mite de steps** â€” el modelo para cuando quiere. 50 es un safety net generoso para modo headless.

---

## MEJORA 4 â€” Auto-VerificaciÃ³n Post-Edit

### El Principio

> DespuÃ©s de editar un archivo, ejecutar automÃ¡ticamente linter/tests
> y devolver el resultado al agente para que pueda auto-corregir.

### 4.1 â€” Sistema de Hooks

```python
# src/architect/core/hooks.py
from dataclasses import dataclass

@dataclass
class HookConfig:
    """ConfiguraciÃ³n de un hook post-edit."""
    name: str
    command: str              # Comando a ejecutar
    file_patterns: list[str]  # Globs: ["*.py", "*.js"]
    timeout: int = 15         # Timeout del comando
    enabled: bool = True

class PostEditHooks:
    """
    Ejecuta hooks automÃ¡ticamente despuÃ©s de que el agente edite un archivo.
    Los resultados se devuelven al agente como tool results adicionales.
    """

    def __init__(self, hooks: list[HookConfig], workspace_root: Path):
        self.hooks = [h for h in hooks if h.enabled]
        self.root = workspace_root

    def run_for_file(self, file_path: str) -> ToolCallResult | None:
        """
        Ejecuta hooks que matchean el archivo editado.
        Retorna el resultado combinado, o None si no hay hooks que apliquen.
        """
        matching = [
            h for h in self.hooks
            if self._matches(file_path, h.file_patterns)
        ]
        if not matching:
            return None

        combined_output = []
        any_failed = False

        for hook in matching:
            try:
                result = subprocess.run(
                    hook.command,
                    shell=True,
                    cwd=str(self.root),
                    capture_output=True,
                    text=True,
                    timeout=hook.timeout,
                    stdin=subprocess.DEVNULL,
                    env={**os.environ, "ARCHITECT_EDITED_FILE": file_path},
                )
                if result.returncode != 0:
                    any_failed = True
                    combined_output.append(
                        f"âš ï¸ Hook '{hook.name}' fallÃ³ (exit {result.returncode}):\n"
                        f"{result.stdout[-500:]}\n{result.stderr[-300:]}"
                    )
                else:
                    combined_output.append(f"âœ“ Hook '{hook.name}': OK")
            except subprocess.TimeoutExpired:
                combined_output.append(f"âš ï¸ Hook '{hook.name}': timeout ({hook.timeout}s)")
                any_failed = True

        if not combined_output:
            return None

        return ToolCallResult(
            tool_name="_auto_verify",
            args={"file": file_path, "hooks": [h.name for h in matching]},
            result=ToolResult(
                success=not any_failed,
                output="\n".join(combined_output),
                error="Algunos hooks de verificaciÃ³n fallaron" if any_failed else None,
            ),
        )

    def _matches(self, file_path: str, patterns: list[str]) -> bool:
        from fnmatch import fnmatch
        return any(fnmatch(file_path, p) for p in patterns)
```

### 4.2 â€” IntegraciÃ³n en Execution Engine

```python
# En execution/engine.py
class ExecutionEngine:
    def __init__(self, ..., hooks: PostEditHooks | None = None):
        self.hooks = hooks

    def run_post_edit_hooks(self, tool_name: str, args: dict) -> ToolCallResult | None:
        """Ejecuta hooks despuÃ©s de una ediciÃ³n. Llamado por el loop."""
        if not self.hooks:
            return None
        file_path = args.get("path")
        if not file_path:
            return None
        return self.hooks.run_for_file(file_path)
```

### 4.3 â€” ConfiguraciÃ³n

```yaml
hooks:
  post_edit:
    - name: "python-lint"
      command: "ruff check $ARCHITECT_EDITED_FILE --no-fix"
      file_patterns: ["*.py"]
      timeout: 10

    - name: "python-typecheck"
      command: "mypy $ARCHITECT_EDITED_FILE --no-error-summary"
      file_patterns: ["*.py"]
      timeout: 15
      enabled: false  # Deshabilitado por defecto

    - name: "js-lint"
      command: "eslint $ARCHITECT_EDITED_FILE"
      file_patterns: ["*.js", "*.ts", "*.jsx", "*.tsx"]
      timeout: 10

    - name: "test-runner"
      command: "pytest --tb=short -q"
      file_patterns: ["*.py"]
      timeout: 30
      enabled: false  # Activar manualmente
```

El hook `_auto_verify` se devuelve al LLM como un tool result mÃ¡s. Si el linter falla, el LLM ve el error y puede corregirlo automÃ¡ticamente en el siguiente step â€” sin que nadie se lo pida.

---

## MEJORA 5 â€” Log Level "Human"

### El Principio

> Un nuevo nivel de log que muestre la trazabilidad del agente
> de forma legible para humanos: quÃ© estÃ¡ haciendo en cada momento,
> con iconos y formato claro, sin ruido tÃ©cnico.

### 5.1 â€” DiseÃ±o del Nivel "Human"

El nivel `human` se sitÃºa entre `info` y `warn` en jerarquÃ­a. Pero conceptualmente es diferente: no indica severidad, indica **trazabilidad de alto nivel**.

```
JerarquÃ­a de niveles:
  debug   â†’ Todo (HTTP payloads, args completos, timing)
  info    â†’ Operaciones del sistema (config loaded, tool registered, etc)
  human   â†’ â˜… Trazabilidad del agente (LLM call, tool use, resultado)
  warn    â†’ Problemas no fatales
  error   â†’ Errores
```

Lo que muestra `human` y lo que no:

| Muestra | No muestra |
|---------|-----------|
| Llamada al LLM (paso N) | Payload HTTP |
| Resultado LLM (OK/error) | Contenido completo de la respuesta |
| Tool invocada (nombre + path/resumen) | Argumentos completos |
| Resultado de tool (OK/error + resumen) | Output completo de la tool |
| Safety net activado | Detalles internos del context manager |
| Agente terminÃ³ (razÃ³n) | Estimaciones de tokens |
| Coste acumulado (si tracking activo) | Detalles de pricing |

### 5.2 â€” Formato Visual

```
â”€â”€â”€ architect Â· build Â· gpt-4.1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ”„ Paso 1 â†’ Llamada al LLM (3 mensajes)
   âœ“ LLM respondiÃ³ con 2 tool calls

   ðŸ”§ read_file â†’ src/main.py
      âœ“ OK (142 lÃ­neas)

   ðŸ”§ read_file â†’ src/config.py
      âœ“ OK (89 lÃ­neas)

ðŸ”„ Paso 2 â†’ Llamada al LLM (7 mensajes)
   âœ“ LLM respondiÃ³ con 1 tool call

   ðŸ”§ edit_file â†’ src/main.py
      âœ“ Editado (+5 -3 lÃ­neas)
      ðŸ” Hook python-lint: OK
      ðŸ” Hook python-typecheck: 1 error
         â†’ src/main.py:45: error: Argument 1 has incompatible type

ðŸ”„ Paso 3 â†’ Llamada al LLM (10 mensajes)
   âœ“ LLM respondiÃ³ con 1 tool call

   ðŸ”§ edit_file â†’ src/main.py
      âœ“ Editado (+2 -1 lÃ­neas)
      ðŸ” Hook python-lint: OK
      ðŸ” Hook python-typecheck: OK

ðŸ”„ Paso 4 â†’ Llamada al LLM (13 mensajes)
   âœ“ LLM respondiÃ³ con texto final

âœ… Agente completado (4 pasos)
   RazÃ³n: LLM decidiÃ³ que terminÃ³
   Coste: $0.0234 (12,450 tokens in / 3,200 out)

â”€â”€â”€ Resultado â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
He modificado src/main.py para aÃ±adir validaciÃ³n
de tipos en la funciÃ³n process_data()...
```

Con MCP tools se diferencia visualmente:

```
   ðŸ”§ read_file â†’ src/main.py           (tool local)
   ðŸŒ mcp_tools1_search â†’ "auth utils"   (tool MCP: tools1)
```

Cuando un safety net salta:

```
âš ï¸  LÃ­mite de pasos alcanzado (50/50)
    Pidiendo al agente que resuma...

ðŸ”„ Cierre â†’ Llamada al LLM (sin tools)
   âœ“ LLM respondiÃ³ con resumen

âš¡ Agente detenido parcialmente (50 pasos)
   RazÃ³n: LÃ­mite de pasos
   Coste: $0.1523
```

### 5.3 â€” ImplementaciÃ³n

```python
# src/architect/logging/levels.py
import logging

# Nivel custom entre INFO (20) y WARNING (30)
HUMAN = 25
logging.addLevelName(HUMAN, "HUMAN")


def human(self, message, *args, **kwargs):
    """Logger method para nivel HUMAN."""
    if self.isEnabledFor(HUMAN):
        self._log(HUMAN, message, *args, **kwargs)


# Monkey-patch en Logger
logging.Logger.human = human
```

```python
# src/architect/logging/human.py
import sys
from typing import Any

class HumanFormatter:
    """
    Formateador de logs nivel HUMAN.
    Produce output legible con iconos y estructura clara.
    """

    # Iconos por tipo de evento
    ICONS = {
        "llm.call":        "ðŸ”„",
        "llm.response":    "   âœ“",
        "llm.error":       "   âœ—",
        "tool.call":       "   ðŸ”§",
        "tool.call.mcp":   "   ðŸŒ",
        "tool.result":     "      âœ“",
        "tool.result.err": "      âœ—",
        "hook.result":     "      ðŸ”",
        "hook.result.err": "      ðŸ”",
        "safety.max_steps":    "âš ï¸ ",
        "safety.budget":       "âš ï¸ ",
        "safety.timeout":      "âš ï¸ ",
        "safety.context_full": "âš ï¸ ",
        "agent.done":      "âœ…",
        "agent.closing":   "âš¡",
        "agent.failed":    "âŒ",
        "context.compress":"   ðŸ“¦",
    }

    def __init__(self, show_costs: bool = True):
        self.show_costs = show_costs
        self.current_step = -1

    def format(self, event: str, **kw) -> str | None:
        """Formatea un evento a texto legible. Retorna None si no aplica."""

        match event:

            # â”€â”€â”€ LLM â”€â”€â”€
            case "llm.call":
                step = kw.get("step", "?")
                msgs = kw.get("messages_count", "?")
                if step != self.current_step:
                    self.current_step = step
                    return f"\nðŸ”„ Paso {step + 1} â†’ Llamada al LLM ({msgs} mensajes)"
                return None

            case "llm.response":
                tool_count = kw.get("tool_calls", 0)
                if tool_count:
                    return f"   âœ“ LLM respondiÃ³ con {tool_count} tool call{'s' if tool_count > 1 else ''}"
                return "   âœ“ LLM respondiÃ³ con texto final"

            case "llm.error":
                return f"   âœ— Error del LLM: {kw.get('error', 'desconocido')}"

            # â”€â”€â”€ TOOLS â”€â”€â”€
            case "tool.call":
                tool = kw.get("tool", "?")
                summary = kw.get("args_summary", "")
                is_mcp = kw.get("is_mcp", False)

                if is_mcp:
                    server = kw.get("mcp_server", "")
                    return f"   ðŸŒ {tool} â†’ {summary}  (MCP: {server})"
                return f"   ðŸ”§ {tool} â†’ {summary}"

            case "tool.result":
                tool = kw.get("tool", "?")
                ok = kw.get("success", False)
                detail = kw.get("detail", "")
                icon = "âœ“" if ok else "âœ—"
                line = f"      {icon} {detail}" if detail else f"      {icon} {'OK' if ok else 'Error'}"
                return line

            # â”€â”€â”€ HOOKS â”€â”€â”€
            case "hook.result":
                hook = kw.get("hook", "?")
                ok = kw.get("success", True)
                icon = "âœ“" if ok else "âš ï¸"
                detail = kw.get("detail", "")
                return f"      ðŸ” Hook {hook}: {icon} {detail}".rstrip()

            # â”€â”€â”€ SAFETY â”€â”€â”€
            case "safety.max_steps":
                step = kw.get("step", "?")
                mx = kw.get("max", "?")
                return f"\nâš ï¸  LÃ­mite de pasos alcanzado ({step}/{mx})\n    Pidiendo al agente que resuma..."

            case "safety.budget":
                spent = kw.get("spent", 0)
                budget = kw.get("budget", 0)
                return f"\nâš ï¸  Presupuesto excedido (${spent:.4f} / ${budget:.4f})\n    Pidiendo al agente que resuma..."

            case "safety.timeout":
                return "\nâš ï¸  Timeout alcanzado\n    Pidiendo al agente que resuma..."

            # â”€â”€â”€ AGENT LIFECYCLE â”€â”€â”€
            case "agent.done":
                step = kw.get("step", "?")
                return f"\nâœ… Agente completado ({step} pasos)\n   RazÃ³n: LLM decidiÃ³ que terminÃ³"

            case "agent.closing":
                reason = kw.get("reason", "?")
                return f"\nðŸ”„ Cierre â†’ Llamada al LLM (sin tools)"

            case "agent.partial":
                steps = kw.get("steps", "?")
                reason = kw.get("reason", "?")
                cost_line = ""
                if self.show_costs and kw.get("cost"):
                    cost_line = f"\n   Coste: ${kw['cost']:.4f}"
                return f"\nâš¡ Agente detenido parcialmente ({steps} pasos)\n   RazÃ³n: {reason}{cost_line}"

            # â”€â”€â”€ CONTEXT â”€â”€â”€
            case "context.compress":
                before = kw.get("before_tokens", "?")
                after = kw.get("after_tokens", "?")
                return f"   ðŸ“¦ Contexto comprimido ({before} â†’ {after} tokens)"

            case _:
                return None


class HumanLogHandler(logging.Handler):
    """Handler que filtra solo eventos HUMAN y los formatea."""

    def __init__(self, stream=None, show_costs=True):
        super().__init__(level=HUMAN)
        self.stream = stream or sys.stderr
        self.formatter = HumanFormatter(show_costs=show_costs)

    def emit(self, record):
        try:
            event = getattr(record, "event", record.getMessage())
            kw = getattr(record, "kw", {})
            formatted = self.formatter.format(event, **kw)
            if formatted:
                self.stream.write(formatted + "\n")
                self.stream.flush()
        except Exception:
            self.handleError(record)
```

### 5.4 â€” IntegraciÃ³n con structlog

```python
# src/architect/logging/setup.py
import structlog
import logging
from .levels import HUMAN
from .human import HumanLogHandler

def configure_logging(
    config: LoggingConfig,
    json_output: bool,
    quiet: bool,
    show_costs: bool = True,
):
    """
    Configura tres pipelines de logging:

    1. Archivo JSON (si configurado) â†’ Todo, estructurado
    2. Human handler (stderr) â†’ Solo eventos de trazabilidad del agente
    3. Console handler (stderr) â†’ Debug/info tÃ©cnico (controlado por -v)
    """

    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.handlers.clear()

    # â”€â”€â”€ 1. Archivo JSON (siempre, si configurado) â”€â”€â”€
    if config.file:
        file_handler = logging.FileHandler(str(config.file))
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter(
            '%(message)s'  # structlog ya formatea como JSON
        ))
        root_logger.addHandler(file_handler)

    # â”€â”€â”€ 2. Human handler (el nuevo) â”€â”€â”€
    if not quiet and not json_output:
        human_handler = HumanLogHandler(show_costs=show_costs)
        # Solo pasa eventos nivel HUMAN (25), no INFO, no DEBUG
        human_handler.setLevel(HUMAN)
        human_handler.addFilter(lambda record: record.levelno == HUMAN)
        root_logger.addHandler(human_handler)

    # â”€â”€â”€ 3. Console tÃ©cnico (controlado por -v) â”€â”€â”€
    if not quiet and not json_output:
        console_handler = logging.StreamHandler(sys.stderr)
        verbose_level = _verbose_to_level(config.verbose)
        console_handler.setLevel(verbose_level)
        # Excluir eventos HUMAN del console handler (ya los muestra human_handler)
        console_handler.addFilter(lambda record: record.levelno != HUMAN)
        root_logger.addHandler(console_handler)

    # Configurar structlog
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
    )


def _verbose_to_level(verbose: int) -> int:
    """
    Sin -v  â†’ Solo human (no se ve info ni debug tÃ©cnico)
    -v      â†’ INFO (steps internos, config, etc)
    -vv     â†’ DEBUG (args completos, HTTP, timing)
    """
    match verbose:
        case 0:
            return logging.WARNING  # Solo warnings/errors (human va por otro handler)
        case 1:
            return logging.INFO
        case _:
            return logging.DEBUG
```

### 5.5 â€” CÃ³mo Emitir Logs Human desde el CÃ³digo

Para emitir un log `human` con structlog:

```python
# OpciÃ³n A: usar el nivel directamente
import structlog
log = structlog.get_logger()

# Dentro del agent loop:
log.log(HUMAN, "llm.call", step=step, messages_count=len(messages))
log.log(HUMAN, "tool.call", tool="read_file", args_summary="src/main.py", is_mcp=False)
log.log(HUMAN, "tool.result", tool="read_file", success=True, detail="142 lÃ­neas")
```

Alternativa mÃ¡s limpia con un wrapper:

```python
# src/architect/logging/human.py (aÃ±adir al final)

class HumanLog:
    """Helper para emitir logs de nivel HUMAN con semÃ¡ntica clara."""

    def __init__(self, logger: structlog.BoundLogger):
        self._log = logger

    def llm_call(self, step: int, messages_count: int):
        self._log.log(HUMAN, "llm.call", step=step, messages_count=messages_count)

    def llm_response(self, tool_calls: int = 0):
        self._log.log(HUMAN, "llm.response", tool_calls=tool_calls)

    def llm_error(self, error: str):
        self._log.log(HUMAN, "llm.error", error=error)

    def tool_call(self, name: str, args_summary: str, is_mcp: bool = False,
                  mcp_server: str = ""):
        self._log.log(HUMAN, "tool.call", tool=name, args_summary=args_summary,
                       is_mcp=is_mcp, mcp_server=mcp_server)

    def tool_result(self, name: str, success: bool, detail: str = ""):
        self._log.log(HUMAN, "tool.result", tool=name, success=success, detail=detail)

    def hook_result(self, hook: str, success: bool, detail: str = ""):
        self._log.log(HUMAN, "hook.result", hook=hook, success=success, detail=detail)

    def safety_net(self, reason: str, **kw):
        self._log.log(HUMAN, f"safety.{reason}", **kw)

    def agent_done(self, step: int):
        self._log.log(HUMAN, "agent.done", step=step)

    def agent_partial(self, steps: int, reason: str, cost: float | None = None):
        self._log.log(HUMAN, "agent.partial", steps=steps, reason=reason, cost=cost)

    def context_compress(self, before_tokens: int, after_tokens: int):
        self._log.log(HUMAN, "context.compress",
                       before_tokens=before_tokens, after_tokens=after_tokens)
```

Uso en el loop:

```python
class AgentLoop:
    def __init__(self, ...):
        self.hlog = HumanLog(structlog.get_logger())

    def run(self, prompt):
        # ...
        self.hlog.llm_call(step=step, messages_count=len(state.messages))
        response = self.llm.completion(...)
        self.hlog.llm_response(tool_calls=len(response.tool_calls))

        for tc in response.tool_calls:
            self.hlog.tool_call(
                name=tc.name,
                args_summary=self._summarize_args(tc.arguments),
                is_mcp=tc.name.startswith("mcp_"),
                mcp_server=tc.name.split("_")[1] if tc.name.startswith("mcp_") else "",
            )
```

### 5.6 â€” QuÃ© Nivel Se Ve Con Cada Flag

| Flag | Ve HUMAN | Ve INFO | Ve DEBUG | Ve archivos JSON |
|------|----------|---------|----------|------------------|
| (ninguno) | âœ… | âŒ | âŒ | Si configurado |
| `-v` | âœ… | âœ… | âŒ | Si configurado |
| `-vv` | âœ… | âœ… | âœ… | Si configurado |
| `--quiet` | âŒ | âŒ | âŒ | Si configurado |
| `--json` | âŒ | âŒ | âŒ | Si configurado |
| `--log-level human` | âœ… | âŒ | âŒ | Si configurado |

**Comportamiento por defecto (sin flags)**: El usuario solo ve los logs `human`. Es la experiencia ideal para seguir quÃ© hace el agente sin ruido.

### 5.7 â€” Banner Inicial y Resultado Final

```python
# Al inicio de la ejecuciÃ³n:
def print_banner(agent: str, model: str):
    """Banner human-readable al inicio."""
    print(f"\nâ”€â”€â”€ architect Â· {agent} Â· {model} {'â”€' * (40 - len(agent) - len(model))}\n",
          file=sys.stderr)

# Al final:
def print_result_separator():
    print(f"\nâ”€â”€â”€ Resultado {'â”€' * 40}\n", file=sys.stderr)
```

---

## MEJORA 6 â€” Ajustes Args Summarizer para Logs

Para que los logs human sean legibles, necesitamos un helper que resuma los argumentos de cada tool de forma inteligente:

```python
# En core/loop.py (o en logging/human.py)

def _summarize_args(self, tool_name: str, args: dict) -> str:
    """
    Resume args de una tool para el log human.
    Cada tool tiene su resumen Ã³ptimo.
    """
    match tool_name:
        case "read_file" | "delete_file":
            return args.get("path", "?")

        case "write_file":
            path = args.get("path", "?")
            content = args.get("content", "")
            lines = content.count("\n") + 1
            return f"{path} ({lines} lÃ­neas)"

        case "edit_file":
            path = args.get("path", "?")
            old = args.get("old_content", "")
            new = args.get("new_content", "")
            return f"{path} ({len(old.splitlines())}â†’{len(new.splitlines())} lÃ­neas)"

        case "apply_patch":
            path = args.get("path", "?")
            patch = args.get("patch", "")
            added = sum(1 for l in patch.splitlines() if l.startswith("+") and not l.startswith("+++"))
            removed = sum(1 for l in patch.splitlines() if l.startswith("-") and not l.startswith("---"))
            return f"{path} (+{added} -{removed})"

        case "search_code":
            return f'"{args.get("pattern", "?")}" en {args.get("path", ".")}'

        case "grep":
            return f'"{args.get("text", "?")}" en {args.get("path", ".")}'

        case "list_files" | "find_files":
            return args.get("path", args.get("pattern", "."))

        case "run_command":
            cmd = args.get("command", "?")
            if len(cmd) > 60:
                cmd = cmd[:57] + "..."
            return cmd

        case _:
            # MCP u otra tool â€” mostrar primer arg o resumen genÃ©rico
            first_val = next(iter(args.values()), "")
            if isinstance(first_val, str) and len(first_val) > 60:
                first_val = first_val[:57] + "..."
            return str(first_val) if first_val else "(sin args)"
```

---

## IntegraciÃ³n: CÃ³mo Queda el Flujo Completo

```python
# src/architect/cli.py â€” Flujo principal simplificado

@main.command()
def run(prompt, **kwargs):
    # 1. Config
    config = load_config(kwargs)

    # 2. Logging (con human level)
    configure_logging(config.logging, kwargs["json_output"], kwargs["quiet"])
    print_banner(agent_name, config.llm.model)

    # 3. LLM
    llm = LLMAdapter(config.llm)

    # 4. Tools
    registry = ToolRegistry()
    register_filesystem_tools(registry, config.workspace)
    register_search_tools(registry, config.workspace)
    register_command_tool(registry, config.commands)

    # 5. MCP (si habilitado)
    if not kwargs["disable_mcp"]:
        MCPDiscovery().discover_and_register(config.mcp.servers, registry)

    # 6. Hooks post-edit
    hooks = PostEditHooks(config.hooks.post_edit, config.workspace.root)

    # 7. Context Manager (NUEVO â€” integrado desde el core)
    context_mgr = ContextManager(config.context, llm)

    # 8. Cost Tracker
    cost_tracker = CostTracker(budget_usd=kwargs.get("budget")) if config.costs.enabled else None

    # 9. Agent Config
    agent_config = resolve_agent(kwargs["agent"], config.agents)
    if kwargs.get("mode"):
        agent_config = agent_config.model_copy(update={"confirm_mode": kwargs["mode"]})

    # 10. Execution Engine
    engine = ExecutionEngine(
        registry=registry,
        config=config,
        confirm_mode=agent_config.confirm_mode,
        hooks=hooks,
    )
    engine.dry_run = kwargs["dry_run"]

    # 11. Agent Loop (NUEVO â€” while True)
    shutdown = GracefulShutdown()
    loop = AgentLoop(
        llm=llm,
        engine=engine,
        agent_config=agent_config,
        context_mgr=context_mgr,
        cost_tracker=cost_tracker,
        shutdown=shutdown,
        logger=structlog.get_logger(),
        timeout=kwargs.get("timeout"),
    )

    # 12. Ejecutar
    state = loop.run(prompt)

    # 13. Output
    print_result_separator()
    if kwargs["json_output"]:
        output = state.to_output_dict()
        if cost_tracker:
            output["costs"] = cost_tracker.summary()
        print(json.dumps(output, indent=2))
    elif not kwargs["quiet"]:
        print(state.final_output or "Sin resultado.")
        if cost_tracker and config.logging.verbose >= 0:
            c = cost_tracker.summary()
            print(
                f"\n   Coste: ${c['total_cost_usd']:.4f} "
                f"({c['total_input_tokens']} in / {c['total_output_tokens']} out)",
                file=sys.stderr,
            )

    # 14. Exit code
    exit_codes = {
        "success": 0,
        "partial": 2,
        "failed": 1,
    }
    sys.exit(exit_codes.get(state.status, 1))
```

---

## Cronograma de Esta v3

Estas mejoras reemplazan y re-orderan varias fases del plan original.

| Mejora v3 | Reemplaza | DÃ­as | CuÃ¡ndo |
|-----------|-----------|------|--------|
| M1: Loop while True + safety nets | F2 (parcial) + F7 | 2 | DÃ­a 3-4 (con F2) |
| M2: ContextManager integrado | F11 (absorbido) | 1 | DÃ­a 4-5 (con F2) |
| M3: Plan integrado en build | F3 (simplificado) | 0.5 | DÃ­a 5 (con F3) |
| M4: Post-edit hooks | Nuevo | 1 | DÃ­a 6 (con F1) |
| M5: Log level human | F5 (ampliado) | 1.5 | DÃ­a 7-8 (con F5) |
| M6: Args summarizer | Nuevo (parte de M5) | 0.5 | DÃ­a 8 (con M5) |

**Ahorro neto**: F7 y F11 se absorben en M1 y M2. F3 se simplifica.

### Nuevo Cronograma Completo

```
F0  Scaffolding + Config            1 dÃ­a      DÃ­a 1
F1  Tools + Engine + Hooks (M4)     3 dÃ­as     DÃ­a 2-4
F2  LLM + Loop (M1) + Context (M2) 3 dÃ­as     DÃ­a 4-6
F3  Agentes (M3 integrado)         0.5 dÃ­as   DÃ­a 7
F5  Logging + Human (M5+M6)         2 dÃ­as     DÃ­a 7-8
F9  Diff inteligente                 3 dÃ­as     DÃ­a 9-11
F10 Contexto inteligente (indexer)   3 dÃ­as     DÃ­a 12-14
F13 run_command                      2 dÃ­as     DÃ­a 14-15
F4  MCP                             2 dÃ­as     DÃ­a 16-17
F6  Streaming + Output              1 dÃ­a      DÃ­a 18
F12 Self-eval (opcional)             2 dÃ­as     DÃ­a 19-20
F14 Cost + Cache                     2 dÃ­as     DÃ­a 21-22
F8  IntegraciÃ³n + docs              1 dÃ­a      DÃ­a 23
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                              ~23 dÃ­as
```

Nota: Ahorramos ~4 dÃ­as respecto al plan v2 (~27 dÃ­as) porque F7 y F11 se absorben, F3 se simplifica, y la ruta crÃ­tica es mÃ¡s directa.

---

## Dependencias Actualizadas

```
F0 (scaffolding)
 â”œâ”€â”€ F1 (tools + engine + hooks M4)
 â”‚    â”œâ”€â”€ F9 (diff inteligente)
 â”‚    â”œâ”€â”€ F10 (contexto inteligente)
 â”‚    â”œâ”€â”€ F13 (run_command)
 â”‚    â””â”€â”€ F2 (LLM + loop M1 + context M2)  â† CORE CRÃTICO
 â”‚         â”œâ”€â”€ F3 (agentes M3)
 â”‚         â”œâ”€â”€ F6 (streaming)
 â”‚         â”œâ”€â”€ F12 (self-eval)
 â”‚         â””â”€â”€ F14 (cost + cache)
 â”œâ”€â”€ F4 (MCP) â† requiere F1
 â””â”€â”€ F5 (logging + human M5) â† puede ser paralelo

F8 (integraciÃ³n) â† requiere todo
```

**Ruta crÃ­tica**: F0 â†’ F1 â†’ F2 (con M1+M2) â†’ F3 (con M3) â†’ F9 â†’ F10 â†’ F13

---

## Cambios Menores en Fases Existentes

### F0 â€” Config Schema

AÃ±adir al schema de Pydantic:

```python
class ContextConfig(BaseModel):
    max_tool_result_tokens: int = 2000
    compress_threshold: float = 0.75
    keep_recent_steps: int = 4
    max_context_tokens: int = 100_000

class HookConfig(BaseModel):
    name: str
    command: str
    file_patterns: list[str]
    timeout: int = 15
    enabled: bool = True

class HooksConfig(BaseModel):
    post_edit: list[HookConfig] = Field(default_factory=list)

class LoggingConfig(BaseModel):
    level: Literal["debug", "info", "human", "warn", "error"] = "human"  # â† default cambiado
    file: Path | None = None
    verbose: int = 0
```

### F1 â€” Tool Registry

Cada tool debe implementar un mÃ©todo `summarize_args()` que el HumanLog usa:

```python
class BaseTool(ABC):
    # ... existente ...

    def summarize_args(self, args: dict) -> str:
        """Resumen legible de los argumentos para logs human."""
        return str(next(iter(args.values()), ""))
```

### F4 â€” MCP Tools

Las MCP tools deben marcarse para que los logs human las distingan:

```python
class MCPToolAdapter(BaseTool):
    is_mcp = True
    mcp_server_name: str  # Para el log human
```

### F14 â€” Cost Tracker

El cost tracker debe poder reportar coste acumulado al HumanLog al final de cada step:

```python
class CostTracker:
    def record(self, ...):
        # ... existente ...
        # Emitir log human si hay budget configurado y estamos al >70%
        if self.budget_usd and self.total_cost_usd > self.budget_usd * 0.7:
            self.log.log(HUMAN, "cost.warning",
                         spent=self.total_cost_usd, budget=self.budget_usd)
```

---

## Config YAML Ejemplo Completo (v3)

```yaml
llm:
  provider: litellm
  model: gpt-4.1
  api_base: http://localhost:8000
  api_key_env: LITELLM_API_KEY
  timeout: 60
  retries: 2
  stream: true
  prompt_caching: true

agents:
  build:
    confirm_mode: confirm-sensitive
    max_steps: 50
    # system_prompt se usa el default (BUILD_PROMPT)
    # allowed_tools se usa el default (todas)

  plan:
    confirm_mode: yolo
    max_steps: 20

  # Agente custom
  deploy:
    system_prompt: "Eres un agente de deployment..."
    allowed_tools: [read_file, run_command, search_code]
    confirm_mode: confirm-all
    max_steps: 15

context:
  max_tool_result_tokens: 2000
  compress_threshold: 0.75
  keep_recent_steps: 4
  max_context_tokens: 100000

logging:
  level: human          # â† nuevo default
  file: ~/.architect/logs.json
  verbose: 0

workspace:
  root: .
  allow_delete: true

hooks:
  post_edit:
    - name: python-lint
      command: "ruff check $ARCHITECT_EDITED_FILE --no-fix"
      file_patterns: ["*.py"]
      timeout: 10

    - name: test-runner
      command: "pytest --tb=short -q"
      file_patterns: ["*.py"]
      timeout: 30
      enabled: false

commands:
  enabled: true
  default_timeout: 30
  max_output_lines: 200

mcp:
  servers:
    - name: tools1
      url: https://mcp.example.com
      token_env: MCP_TOKEN

indexer:
  enabled: true
  max_file_size: 1000000

costs:
  enabled: true
  budget_usd: null
  warn_at_usd: null

evaluation:
  mode: "off"

cache:
  enabled: false
  dir: ~/.architect/cache
```

---

## Resumen de Todo Lo Que Cambia

```
CORE (cambios fundamentales):
  âœ“ while True (LLM decide) en vez de for-range
  âœ“ Cierre limpio en todos los safety nets
  âœ“ ContextManager integrado desde el core
  âœ“ Plan integrado en build (no fases separadas)
  âœ“ MixedModeRunner eliminado

LOGGING (nuevo):
  âœ“ Nivel HUMAN (25) entre INFO y WARNING
  âœ“ HumanFormatter con iconos y formato legible
  âœ“ HumanLog helper para emitir eventos tipados
  âœ“ DistinciÃ³n visual local vs MCP
  âœ“ Banner + separador de resultado
  âœ“ Default: solo human logs (sin -v)

AUTO-VERIFY (nuevo):
  âœ“ PostEditHooks ejecuta lint/test despuÃ©s de editar
  âœ“ Resultados vuelven al LLM como tool results
  âœ“ Configurable por file pattern
  âœ“ Deshabilitables individualmente

ARGS SUMMARIZER (nuevo):
  âœ“ Cada tool produce un resumen legible de sus args
  âœ“ Usado por HumanLog para logs concisos
```
